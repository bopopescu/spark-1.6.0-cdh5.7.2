From 8f00d5fc78fec323f9f7146cc5de88c3ca6436e2 Mon Sep 17 00:00:00 2001
From: Marcelo Vanzin <vanzin@cloudera.com>
Date: Thu, 11 Feb 2016 09:43:04 -0800
Subject: [PATCH 127/201] CLOUDERA-BUILD. Preview of "[SPARK-5682] Add
 encrypted shuffle in spark".

This patch is using Chimera library to enable shuffle encryption support.
---
 core/pom.xml                                       |    4 +
 .../main/scala/org/apache/spark/SparkContext.scala |    5 +
 .../scala/org/apache/spark/crypto/CryptoConf.scala |   71 +++++
 .../apache/spark/crypto/CryptoStreamUtils.scala    |   99 +++++++
 .../spark/shuffle/BlockStoreShuffleReader.scala    |   18 +-
 .../org/apache/spark/storage/BlockManager.scala    |    2 +-
 .../spark/storage/DiskBlockObjectWriter.scala      |   25 +-
 .../spark/crypto/ShuffleEncryptionSuite.scala      |  109 +++++++
 .../sort/BypassMergeSortShuffleWriterSuite.scala   |    3 +-
 dev/deps/spark-deps-hadoop-1                       |    1 +
 dev/deps/spark-deps-hadoop-2.2                     |    1 +
 dev/deps/spark-deps-hadoop-2.3                     |    1 +
 dev/deps/spark-deps-hadoop-2.4                     |    1 +
 dev/deps/spark-deps-hadoop-2.6                     |    1 +
 docs/configuration.md                              |   51 ++++
 pom.xml                                            |    6 +
 .../org/apache/spark/deploy/yarn/Client.scala      |   11 +-
 .../deploy/yarn/YarnShuffleEncryptionSuite.scala   |  297 ++++++++++++++++++++
 18 files changed, 693 insertions(+), 13 deletions(-)
 create mode 100644 core/src/main/scala/org/apache/spark/crypto/CryptoConf.scala
 create mode 100644 core/src/main/scala/org/apache/spark/crypto/CryptoStreamUtils.scala
 create mode 100644 core/src/test/scala/org/apache/spark/crypto/ShuffleEncryptionSuite.scala
 create mode 100644 yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnShuffleEncryptionSuite.scala

diff --git a/core/pom.xml b/core/pom.xml
index fc0d172..2b05d28 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -356,6 +356,10 @@
       <groupId>org.apache.spark</groupId>
       <artifactId>spark-test-tags_${scala.binary.version}</artifactId>
     </dependency>
+    <dependency>
+      <groupId>com.intel.chimera</groupId>
+      <artifactId>chimera</artifactId>
+    </dependency>
   </dependencies>
   <build>
     <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>
diff --git a/core/src/main/scala/org/apache/spark/SparkContext.scala b/core/src/main/scala/org/apache/spark/SparkContext.scala
index f590747..8614cc7 100644
--- a/core/src/main/scala/org/apache/spark/SparkContext.scala
+++ b/core/src/main/scala/org/apache/spark/SparkContext.scala
@@ -47,6 +47,7 @@ import org.apache.mesos.MesosNativeLibrary
 
 import org.apache.spark.annotation.DeveloperApi
 import org.apache.spark.broadcast.Broadcast
+import org.apache.spark.crypto.CryptoConf
 import org.apache.spark.deploy.{LocalSparkCluster, SparkHadoopUtil}
 import org.apache.spark.input.{StreamInputFormat, PortableDataStream, WholeTextFileInputFormat,
   FixedLengthBinaryInputFormat}
@@ -447,6 +448,10 @@ class SparkContext(config: SparkConf) extends Logging with ExecutorAllocationCli
     _conf.set("spark.externalBlockStore.folderName", externalBlockStoreFolderName)
 
     if (master == "yarn-client") System.setProperty("SPARK_YARN_MODE", "true")
+    if (CryptoConf.isShuffleEncryptionEnabled(_conf) && !SparkHadoopUtil.get.isYarnMode()) {
+      throw new SparkException("Shuffle file encryption is only supported in Yarn mode, please " +
+        "disable it by setting spark.shuffle.encryption.enabled to false")
+    }
 
     // "_jobProgressListener" should be set up before creating SparkEnv because when creating
     // "SparkEnv", some messages will be posted to "listenerBus" and we should not miss them.
diff --git a/core/src/main/scala/org/apache/spark/crypto/CryptoConf.scala b/core/src/main/scala/org/apache/spark/crypto/CryptoConf.scala
new file mode 100644
index 0000000..3a9dc22
--- /dev/null
+++ b/core/src/main/scala/org/apache/spark/crypto/CryptoConf.scala
@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.spark.crypto
+
+import javax.crypto.KeyGenerator
+
+import org.apache.hadoop.io.Text
+import org.apache.hadoop.security.Credentials
+
+import org.apache.spark.SparkConf
+
+/**
+ * CryptoConf is a class for Crypto configuration
+ */
+private[spark] object CryptoConf {
+  /**
+   * Constants and variables for spark shuffle file encryption
+   */
+  val SPARK_SHUFFLE_TOKEN = new Text("SPARK_SHUFFLE_TOKEN")
+  val SPARK_SHUFFLE_ENCRYPTION_ENABLED = "spark.shuffle.encryption.enabled"
+  val SPARK_SHUFFLE_ENCRYPTION_KEYGEN_ALGORITHM = "spark.shuffle.encryption.keygen.algorithm"
+  val DEFAULT_SPARK_SHUFFLE_ENCRYPTION_KEYGEN_ALGORITHM = "HmacSHA1"
+  val SPARK_SHUFFLE_ENCRYPTION_KEY_SIZE_BITS = "spark.shuffle.encryption.keySizeBits"
+  val DEFAULT_SPARK_SHUFFLE_ENCRYPTION_KEY_SIZE_BITS = 128
+
+  /**
+   * Check whether shuffle file encryption is enabled. It is disabled by default.
+   */
+  def isShuffleEncryptionEnabled(sparkConf: SparkConf): Boolean = {
+    if (sparkConf != null) {
+      sparkConf.getBoolean(SPARK_SHUFFLE_ENCRYPTION_ENABLED, false)
+    } else {
+      false
+    }
+  }
+
+  /**
+   * Setup the cryptographic key used by file shuffle encryption in credentials. The key is
+   * generated using [[KeyGenerator]]. The algorithm and key length is specified by the
+   * [[SparkConf]].
+   */
+  def initSparkShuffleCredentials(conf: SparkConf, credentials: Credentials): Unit = {
+    if (credentials.getSecretKey(SPARK_SHUFFLE_TOKEN) == null) {
+      val keyLen = conf.getInt(SPARK_SHUFFLE_ENCRYPTION_KEY_SIZE_BITS,
+        DEFAULT_SPARK_SHUFFLE_ENCRYPTION_KEY_SIZE_BITS)
+      require(keyLen == 128 || keyLen == 192 || keyLen == 256)
+      val shuffleKeyGenAlgorithm = conf.get(SPARK_SHUFFLE_ENCRYPTION_KEYGEN_ALGORITHM,
+        DEFAULT_SPARK_SHUFFLE_ENCRYPTION_KEYGEN_ALGORITHM)
+      val keyGen = KeyGenerator.getInstance(shuffleKeyGenAlgorithm)
+      keyGen.init(keyLen)
+
+      val shuffleKey = keyGen.generateKey()
+      credentials.addSecretKey(SPARK_SHUFFLE_TOKEN, shuffleKey.getEncoded)
+    }
+  }
+}
+
diff --git a/core/src/main/scala/org/apache/spark/crypto/CryptoStreamUtils.scala b/core/src/main/scala/org/apache/spark/crypto/CryptoStreamUtils.scala
new file mode 100644
index 0000000..af706fd
--- /dev/null
+++ b/core/src/main/scala/org/apache/spark/crypto/CryptoStreamUtils.scala
@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.spark.crypto
+
+import java.io.{InputStream, OutputStream}
+import java.util.Properties
+
+import com.intel.chimera.cipher._
+import com.intel.chimera.random._
+import com.intel.chimera.stream._
+
+import org.apache.spark.SparkConf
+import org.apache.spark.crypto.CryptoConf._
+import org.apache.spark.deploy.SparkHadoopUtil
+
+/**
+ * A util class for manipulating file shuffle encryption and decryption streams.
+ */
+private[spark] object CryptoStreamUtils {
+  // The initialization vector length in bytes.
+  val IV_LENGTH_IN_BYTES = 16
+  // The prefix of Crypto related configurations in Spark configuration.
+  val SPARK_CHIMERA_CONF_PREFIX = "spark.shuffle.crypto."
+  // The prefix for the configurations passing to Chimera library.
+  val CHIMERA_CONF_PREFIX = "chimera.crypto."
+
+  /**
+   * Helper method to wrap [[OutputStream]] with [[CryptoOutputStream]] for encryption.
+   */
+  def createCryptoOutputStream(os: OutputStream, sparkConf: SparkConf): CryptoOutputStream = {
+    val properties = toChimeraConf(sparkConf)
+    val iv: Array[Byte] = createInitializationVector(properties)
+    os.write(iv)
+    val credentials = SparkHadoopUtil.get.getCurrentUserCredentials()
+    val key = credentials.getSecretKey(SPARK_SHUFFLE_TOKEN)
+    val transformationType = getCipherTransformationType(sparkConf)
+    new CryptoOutputStream(transformationType, properties, os, key, iv)
+  }
+
+  /**
+   * Helper method to wrap [[InputStream]] with [[CryptoInputStream]] for decryption.
+   */
+  def createCryptoInputStream(is: InputStream, sparkConf: SparkConf): CryptoInputStream = {
+    val properties = toChimeraConf(sparkConf)
+    val iv = new Array[Byte](IV_LENGTH_IN_BYTES)
+    is.read(iv, 0, iv.length)
+    val credentials = SparkHadoopUtil.get.getCurrentUserCredentials()
+    val key = credentials.getSecretKey(SPARK_SHUFFLE_TOKEN)
+    val transformationType = getCipherTransformationType(sparkConf)
+    new CryptoInputStream(transformationType, properties, is, key, iv)
+  }
+
+  /**
+   * Get Chimera configurations from Spark configurations identified by prefix.
+   */
+  def toChimeraConf(conf: SparkConf): Properties = {
+    val props = new Properties()
+    conf.getAll.foreach { case (k, v) =>
+      if (k.startsWith(SPARK_CHIMERA_CONF_PREFIX)) {
+        props.put(CHIMERA_CONF_PREFIX + k.substring(SPARK_CHIMERA_CONF_PREFIX.length()), v)
+      }
+    }
+    props
+  }
+
+  /**
+   * Get the cipher transformation type
+   */
+  private[this] def getCipherTransformationType(sparkConf: SparkConf): CipherTransformation = {
+    val transformationStr = sparkConf.get("spark.shuffle.crypto.cipher.transformation",
+      "AES/CTR/NoPadding")
+    CipherTransformation.values().find(
+      (cipherTransformation: CipherTransformation) => cipherTransformation.getName() ==
+          transformationStr).get
+  }
+
+  /**
+   * This method to generate an IV (Initialization Vector) using secure random.
+   */
+  private[this] def createInitializationVector(properties: Properties): Array[Byte] = {
+    val iv = new Array[Byte](IV_LENGTH_IN_BYTES)
+    SecureRandomFactory.getSecureRandom(properties).nextBytes(iv)
+    iv
+  }
+}
diff --git a/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala b/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala
index b0abda4..50ad282 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala
@@ -18,6 +18,8 @@
 package org.apache.spark.shuffle
 
 import org.apache.spark._
+import org.apache.spark.crypto._
+import org.apache.spark.deploy.SparkHadoopUtil
 import org.apache.spark.serializer.Serializer
 import org.apache.spark.storage.{BlockManager, ShuffleBlockFetcherIterator}
 import org.apache.spark.util.CompletionIterator
@@ -48,14 +50,20 @@ private[spark] class BlockStoreShuffleReader[K, C](
       // Note: we use getSizeAsMb when no suffix is provided for backwards compatibility
       SparkEnv.get.conf.getSizeAsMb("spark.reducer.maxSizeInFlight", "48m") * 1024 * 1024)
 
-    // Wrap the streams for compression based on configuration
-    val wrappedStreams = blockFetcherItr.map { case (blockId, inputStream) =>
-      blockManager.wrapForCompression(blockId, inputStream)
-    }
-
     val ser = Serializer.getSerializer(dep.serializer)
     val serializerInstance = ser.newInstance()
 
+    // Wrap the streams for compression and encryption based on configuration
+    val wrappedStreams = blockFetcherItr.map { case (blockId, inputStream) =>
+      val sparkConf = blockManager.conf
+      if (CryptoConf.isShuffleEncryptionEnabled(sparkConf)) {
+        val cis = CryptoStreamUtils.createCryptoInputStream(inputStream, sparkConf)
+        blockManager.wrapForCompression(blockId, cis)
+      } else {
+        blockManager.wrapForCompression(blockId, inputStream)
+      }
+    }
+
     // Create a key/value iterator for each stream
     val recordIter = wrappedStreams.flatMap { wrappedStream =>
       // Note: the asKeyValueIterator below wraps a key/value iterator inside of a
diff --git a/core/src/main/scala/org/apache/spark/storage/BlockManager.scala b/core/src/main/scala/org/apache/spark/storage/BlockManager.scala
index 2cc2fd9..f15ca36 100644
--- a/core/src/main/scala/org/apache/spark/storage/BlockManager.scala
+++ b/core/src/main/scala/org/apache/spark/storage/BlockManager.scala
@@ -659,7 +659,7 @@ private[spark] class BlockManager(
     val compressStream: OutputStream => OutputStream = wrapForCompression(blockId, _)
     val syncWrites = conf.getBoolean("spark.shuffle.sync", false)
     new DiskBlockObjectWriter(file, serializerInstance, bufferSize, compressStream,
-      syncWrites, writeMetrics, blockId)
+      syncWrites, writeMetrics, blockId, conf)
   }
 
   /**
diff --git a/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala b/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala
index e2dd80f..1802650 100644
--- a/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala
+++ b/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala
@@ -20,7 +20,8 @@ package org.apache.spark.storage
 import java.io.{BufferedOutputStream, FileOutputStream, File, OutputStream}
 import java.nio.channels.FileChannel
 
-import org.apache.spark.Logging
+import org.apache.spark.{Logging, SparkConf}
+import org.apache.spark.crypto._
 import org.apache.spark.serializer.{SerializerInstance, SerializationStream}
 import org.apache.spark.executor.ShuffleWriteMetrics
 import org.apache.spark.util.Utils
@@ -42,10 +43,23 @@ private[spark] class DiskBlockObjectWriter(
     // These write metrics concurrently shared with other active DiskBlockObjectWriters who
     // are themselves performing writes. All updates must be relative.
     writeMetrics: ShuffleWriteMetrics,
-    val blockId: BlockId = null)
+    val blockId: BlockId = null,
+    val sparkConf: SparkConf = null)
   extends OutputStream
   with Logging {
 
+  def this(
+      file: File,
+      serializerInstance: SerializerInstance,
+      bufferSize: Int,
+      compressStream: OutputStream => OutputStream,
+      syncWrites: Boolean,
+      writeMetrics: ShuffleWriteMetrics,
+      blockId: BlockId) {
+    this(file, serializerInstance, bufferSize, compressStream, syncWrites, writeMetrics, blockId,
+      null)
+  }
+
   /** The file channel, used for repositioning / truncating the file. */
   private var channel: FileChannel = null
   private var bs: OutputStream = null
@@ -86,7 +100,12 @@ private[spark] class DiskBlockObjectWriter(
       throw new IllegalStateException("Writer already closed. Cannot be reopened.")
     }
     fos = new FileOutputStream(file, true)
-    ts = new TimeTrackingOutputStream(writeMetrics, fos)
+    if (CryptoConf.isShuffleEncryptionEnabled(sparkConf)) {
+      val cos = CryptoStreamUtils.createCryptoOutputStream(fos, sparkConf)
+      ts = new TimeTrackingOutputStream(writeMetrics, cos)
+    } else {
+      ts = new TimeTrackingOutputStream(writeMetrics, fos)
+    }
     channel = fos.getChannel()
     bs = compressStream(new BufferedOutputStream(ts, bufferSize))
     objOut = serializerInstance.serializeStream(bs)
diff --git a/core/src/test/scala/org/apache/spark/crypto/ShuffleEncryptionSuite.scala b/core/src/test/scala/org/apache/spark/crypto/ShuffleEncryptionSuite.scala
new file mode 100644
index 0000000..12b199d
--- /dev/null
+++ b/core/src/test/scala/org/apache/spark/crypto/ShuffleEncryptionSuite.scala
@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.spark.crypto
+
+import java.security.PrivilegedExceptionAction
+
+import org.apache.hadoop.security.{Credentials, UserGroupInformation}
+
+import org.apache.spark.{SparkConf, SparkFunSuite}
+import org.apache.spark.crypto.CryptoConf._
+import org.apache.spark.crypto.CryptoStreamUtils._
+
+private[spark] class ShuffleEncryptionSuite extends SparkFunSuite {
+  val ugi = UserGroupInformation.createUserForTesting("testuser", Array("testgroup"))
+
+  test("Test Chimera configuration conversion") {
+    val sparkKey1 = s"${SPARK_CHIMERA_CONF_PREFIX}a.b.c"
+    val sparkVal1 = "val1"
+    val chimeraKey1 = s"${CHIMERA_CONF_PREFIX}a.b.c"
+
+    val sparkKey2 = s"${SPARK_CHIMERA_CONF_PREFIX.stripSuffix(".")}A.b.c"
+    val sparkVal2 = "val2"
+    val chimeraKey2 = s"${CHIMERA_CONF_PREFIX}A.b.c"
+
+    val conf = new SparkConf()
+    conf.set(sparkKey1, sparkVal1)
+    conf.set(sparkKey2, sparkVal2)
+
+    val props = toChimeraConf(conf)
+    assert(props.getProperty(chimeraKey1) === sparkVal1)
+    assert(!props.contains(chimeraKey2))
+  }
+
+  test("Test shuffle encryption is disabled by default"){
+    ugi.doAs(new PrivilegedExceptionAction[Unit]() {
+      override def run(): Unit = {
+        val credentials = UserGroupInformation.getCurrentUser.getCredentials()
+        val conf = new SparkConf()
+        initCredentials(conf, credentials)
+        assert(credentials.getSecretKey(SPARK_SHUFFLE_TOKEN) === null)
+      }
+    })
+  }
+
+  test("Test shuffle encryption key length should be 128 by default") {
+    ugi.doAs(new PrivilegedExceptionAction[Unit]() {
+      override def run(): Unit = {
+        val credentials = UserGroupInformation.getCurrentUser.getCredentials()
+        val conf = new SparkConf()
+        conf.set(SPARK_SHUFFLE_ENCRYPTION_ENABLED, true.toString)
+        initCredentials(conf, credentials)
+        var key = credentials.getSecretKey(SPARK_SHUFFLE_TOKEN)
+        assert(key !== null)
+        val actual = key.length * (java.lang.Byte.SIZE)
+        assert(actual === DEFAULT_SPARK_SHUFFLE_ENCRYPTION_KEY_SIZE_BITS)
+      }
+    })
+  }
+
+  test("Test initial credentials with key length in 256") {
+    ugi.doAs(new PrivilegedExceptionAction[Unit]() {
+      override def run(): Unit = {
+        val credentials = UserGroupInformation.getCurrentUser.getCredentials()
+        val conf = new SparkConf()
+        conf.set(SPARK_SHUFFLE_ENCRYPTION_KEY_SIZE_BITS, 256.toString)
+        conf.set(SPARK_SHUFFLE_ENCRYPTION_ENABLED, true.toString)
+        initCredentials(conf, credentials)
+        var key = credentials.getSecretKey(SPARK_SHUFFLE_TOKEN)
+        assert(key !== null)
+        val actual = key.length * (java.lang.Byte.SIZE)
+        assert(actual === 256)
+      }
+    })
+  }
+
+  test("Test initial credentials with invalid key length") {
+    ugi.doAs(new PrivilegedExceptionAction[Unit]() {
+      override def run(): Unit = {
+        val credentials = UserGroupInformation.getCurrentUser.getCredentials()
+        val conf = new SparkConf()
+        conf.set(SPARK_SHUFFLE_ENCRYPTION_KEY_SIZE_BITS, 328.toString)
+        conf.set(SPARK_SHUFFLE_ENCRYPTION_ENABLED, true.toString)
+        val thrown = intercept[IllegalArgumentException] {
+          initCredentials(conf, credentials)
+        }
+      }
+    })
+  }
+
+  private[this] def initCredentials(conf: SparkConf, credentials: Credentials): Unit = {
+    if (CryptoConf.isShuffleEncryptionEnabled(conf)) {
+      CryptoConf.initSparkShuffleCredentials(conf, credentials)
+    }
+  }
+}
diff --git a/core/src/test/scala/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriterSuite.scala b/core/src/test/scala/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriterSuite.scala
index d3b1b2b..164f2bc 100644
--- a/core/src/test/scala/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriterSuite.scala
+++ b/core/src/test/scala/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriterSuite.scala
@@ -96,7 +96,8 @@ class BypassMergeSortShuffleWriterSuite extends SparkFunSuite with BeforeAndAfte
           compressStream = identity,
           syncWrites = false,
           args(4).asInstanceOf[ShuffleWriteMetrics],
-          blockId = args(0).asInstanceOf[BlockId]
+          blockId = args(0).asInstanceOf[BlockId],
+          conf
         )
       }
     })
diff --git a/dev/deps/spark-deps-hadoop-1 b/dev/deps/spark-deps-hadoop-1
index d5e85ad..bd99c45 100644
--- a/dev/deps/spark-deps-hadoop-1
+++ b/dev/deps/spark-deps-hadoop-1
@@ -24,6 +24,7 @@ calcite-core-1.2.0-incubating.jar
 calcite-linq4j-1.2.0-incubating.jar
 chill-java-0.5.0.jar
 chill_2.10-0.5.0.jar
+chimera-0.9.2.jar
 commons-beanutils-1.7.0.jar
 commons-beanutils-core-1.8.0.jar
 commons-cli-1.2.jar
diff --git a/dev/deps/spark-deps-hadoop-2.2 b/dev/deps/spark-deps-hadoop-2.2
index 0364905..70c3522 100644
--- a/dev/deps/spark-deps-hadoop-2.2
+++ b/dev/deps/spark-deps-hadoop-2.2
@@ -25,6 +25,7 @@ calcite-core-1.2.0-incubating.jar
 calcite-linq4j-1.2.0-incubating.jar
 chill-java-0.5.0.jar
 chill_2.10-0.5.0.jar
+chimera-0.9.2.jar
 commons-beanutils-1.7.0.jar
 commons-beanutils-core-1.8.0.jar
 commons-cli-1.2.jar
diff --git a/dev/deps/spark-deps-hadoop-2.3 b/dev/deps/spark-deps-hadoop-2.3
index b3481ac..f80ad76 100644
--- a/dev/deps/spark-deps-hadoop-2.3
+++ b/dev/deps/spark-deps-hadoop-2.3
@@ -27,6 +27,7 @@ calcite-core-1.2.0-incubating.jar
 calcite-linq4j-1.2.0-incubating.jar
 chill-java-0.5.0.jar
 chill_2.10-0.5.0.jar
+chimera-0.9.2.jar
 commons-beanutils-1.7.0.jar
 commons-beanutils-core-1.8.0.jar
 commons-cli-1.2.jar
diff --git a/dev/deps/spark-deps-hadoop-2.4 b/dev/deps/spark-deps-hadoop-2.4
index 7ddff07..5e80490 100644
--- a/dev/deps/spark-deps-hadoop-2.4
+++ b/dev/deps/spark-deps-hadoop-2.4
@@ -27,6 +27,7 @@ calcite-core-1.2.0-incubating.jar
 calcite-linq4j-1.2.0-incubating.jar
 chill-java-0.5.0.jar
 chill_2.10-0.5.0.jar
+chimera-0.9.2.jar
 commons-beanutils-1.7.0.jar
 commons-beanutils-core-1.8.0.jar
 commons-cli-1.2.jar
diff --git a/dev/deps/spark-deps-hadoop-2.6 b/dev/deps/spark-deps-hadoop-2.6
index 4e510bd..e414ce0 100644
--- a/dev/deps/spark-deps-hadoop-2.6
+++ b/dev/deps/spark-deps-hadoop-2.6
@@ -31,6 +31,7 @@ calcite-core-1.2.0-incubating.jar
 calcite-linq4j-1.2.0-incubating.jar
 chill-java-0.5.0.jar
 chill_2.10-0.5.0.jar
+chimera-0.9.2.jar
 commons-beanutils-1.7.0.jar
 commons-beanutils-core-1.8.0.jar
 commons-cli-1.2.jar
diff --git a/docs/configuration.md b/docs/configuration.md
index a151269..44cdc68 100644
--- a/docs/configuration.md
+++ b/docs/configuration.md
@@ -477,6 +477,57 @@ Apart from these, the following properties are also available, and may be useful
     <code>spark.io.compression.codec</code>.
   </td>
 </tr>
+<tr>
+  <td><code>spark.shuffle.encryption.enabled</code></td>
+  <td>false</td>
+  <td>
+    Enable shuffle file encryption.
+  </td>
+</tr>
+<tr>
+  <td><code>spark.shuffle.encryption.keySizeBits</code></td>
+  <td>128</td>
+  <td>
+    Shuffle file encryption key size in bits. The valid number includes 128, 192 and 256.
+  </td>
+</tr>
+<tr>
+  <td><code>spark.shuffle.encryption.keygen.algorithm</code></td>
+  <td>HmacSHA1</td>
+  <td>
+    The algorithm to generate the key used by shuffle file encryption. The supported algorithms are
+    described in the KeyGenerator section of the Java Cryptography Architecture Standard Algorithm
+    Name Documentation.
+  </td>
+</tr>
+<tr>
+  <td><code>spark.shuffle.crypto.cipher.transformation</code></td>
+  <td>AES/CTR/NoPadding</td>
+  <td>
+    Cipher transformation for shuffle file encryption. The cipher transformation name is
+    identical to the transformations described in the Cipher section of the Java Cryptography
+    Architecture Standard Algorithm Name Documentation. Currently only "AES/CTR/NoPadding"
+    algorithm is supported.
+  </td>
+</tr>
+<tr>
+  <td><code>spark.shuffle.crypto.cipher.classes</code></td>
+  <td>com.intel.chimera.cipher.OpensslCipher,com.intel.chimera.cipher.JceCipher</td>
+  <td>
+    Comma-separated list of crypto cipher classes which implement cipher algorithm of
+    "AES/CTR/NoPadding". A crypto cipher implementation encapsulates the encryption and decryption
+    details. The first available implementation appearing in this list will be used.
+  </td>
+</tr>
+<tr>
+  <td><code>spark.shuffle.crypto.secure.random.classes</code></td>
+  <td>com.intel.chimera.random.OpensslSecureRandom,com.intel.chimera.random.OsSecureRandom,com.intel.chimera.random.JavaSecureRandom</td>
+  <td>
+    Comma-separated list of secure random classes which implement secure random algorithm. It's
+    used when generating the Initialization Vector for crypto input/output streams. The first
+    available implementation appearing in this list will be used.
+  </td>
+</tr>
 </table>
 
 #### Spark UI
diff --git a/pom.xml b/pom.xml
index c1a504e..446abee 100644
--- a/pom.xml
+++ b/pom.xml
@@ -189,6 +189,7 @@
     <jsr305.version>1.3.9</jsr305.version>
     <libthrift.version>${cdh.thrift.version}</libthrift.version>
     <spark-avro.version>${cdh.spark-avro.version}</spark-avro.version>
+    <chimera.version>0.9.2</chimera.version>
 
     <test.java.home>${java.home}</test.java.home>
     <test.exclude.tags></test.exclude.tags>
@@ -1904,6 +1905,11 @@
           </exclusion>
         </exclusions>
       </dependency>
+      <dependency>
+        <groupId>com.intel.chimera</groupId>
+        <artifactId>chimera</artifactId>
+        <version>${chimera.version}</version>
+      </dependency>
     </dependencies>
   </dependencyManagement>
 
diff --git a/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala b/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala
index 1102a8c..abfda0c 100644
--- a/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala
+++ b/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala
@@ -19,16 +19,15 @@ package org.apache.spark.deploy.yarn
 
 import java.io.{ByteArrayInputStream, DataInputStream, File, FileOutputStream, IOException,
   OutputStreamWriter}
-import java.net.{InetAddress, UnknownHostException, URI, URISyntaxException}
+import java.net.{InetAddress, UnknownHostException, URI}
 import java.nio.ByteBuffer
-import java.security.PrivilegedExceptionAction
 import java.util.{Properties, UUID}
 import java.util.zip.{ZipEntry, ZipOutputStream}
 
 import scala.collection.JavaConverters._
 import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet, ListBuffer, Map}
 import scala.reflect.runtime.universe
-import scala.util.{Try, Success, Failure}
+import scala.util.{Failure, Success, Try}
 import scala.util.control.NonFatal
 
 import com.google.common.base.Charsets.UTF_8
@@ -55,6 +54,8 @@ import org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException
 import org.apache.hadoop.yarn.util.Records
 
 import org.apache.spark.{Logging, SecurityManager, SparkConf, SparkContext, SparkException}
+import org.apache.spark.crypto.CryptoConf
+import org.apache.spark.deploy.SparkHadoopUtil
 import org.apache.spark.launcher.{LauncherBackend, SparkAppHandle, YarnCommandBuilderUtils}
 import org.apache.spark.deploy.SparkHadoopUtil
 import org.apache.spark.util.Utils
@@ -878,6 +879,10 @@ private[spark] class Client(
     val securityManager = new SecurityManager(sparkConf)
     amContainer.setApplicationACLs(
       YarnSparkHadoopUtil.getApplicationAclsForYarn(securityManager).asJava)
+
+    if (CryptoConf.isShuffleEncryptionEnabled(sparkConf)) {
+      CryptoConf.initSparkShuffleCredentials(sparkConf, credentials)
+    }
     setupSecurityToken(amContainer)
     UserGroupInformation.getCurrentUser().addCredentials(credentials)
 
diff --git a/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnShuffleEncryptionSuite.scala b/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnShuffleEncryptionSuite.scala
new file mode 100644
index 0000000..4053984
--- /dev/null
+++ b/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnShuffleEncryptionSuite.scala
@@ -0,0 +1,297 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.spark.deploy.yarn
+
+import java.io._
+import java.nio.ByteBuffer
+import java.security.PrivilegedExceptionAction
+import java.util.{ArrayList => JArrayList, LinkedList => JLinkedList, UUID}
+
+import scala.runtime.AbstractFunction1
+
+import com.google.common.collect.HashMultiset
+import com.google.common.io.ByteStreams
+import org.apache.hadoop.security.{Credentials, UserGroupInformation}
+import org.junit.Assert.assertEquals
+import org.mockito.Mock
+import org.mockito.MockitoAnnotations
+import org.mockito.invocation.InvocationOnMock
+import org.mockito.stubbing.Answer
+import org.mockito.Answers.RETURNS_SMART_NULLS
+import org.mockito.Matchers.{eq => meq, _}
+import org.mockito.Mockito._
+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfterEach, Matchers}
+
+import org.apache.spark._
+import org.apache.spark.crypto.CryptoConf
+import org.apache.spark.crypto.CryptoConf._
+import org.apache.spark.deploy.SparkHadoopUtil
+import org.apache.spark.executor.{ShuffleWriteMetrics, TaskMetrics}
+import org.apache.spark.io.CompressionCodec
+import org.apache.spark.memory.{TaskMemoryManager, TestMemoryManager}
+import org.apache.spark.network.buffer.NioManagedBuffer
+import org.apache.spark.serializer.{KryoSerializer, SerializerInstance}
+import org.apache.spark.shuffle.{BaseShuffleHandle, BlockStoreShuffleReader,
+  IndexShuffleBlockResolver, RecordingManagedBuffer}
+import org.apache.spark.shuffle.sort.{SerializedShuffleHandle, UnsafeShuffleWriter}
+import org.apache.spark.storage._
+import org.apache.spark.util.Utils
+
+private[spark] class YarnShuffleEncryptionSuite extends SparkFunSuite with Matchers with
+                                                        BeforeAndAfterAll with BeforeAndAfterEach {
+
+  @Mock(answer = RETURNS_SMART_NULLS) private[this] var blockManager: BlockManager = _
+  @Mock(answer = RETURNS_SMART_NULLS) private[this] var blockResolver: IndexShuffleBlockResolver = _
+  @Mock(answer = RETURNS_SMART_NULLS) private[this] var diskBlockManager: DiskBlockManager = _
+  @Mock(answer = RETURNS_SMART_NULLS) private[this] var taskContext: TaskContext = _
+  @Mock(
+    answer = RETURNS_SMART_NULLS) private[this] var shuffleDep: ShuffleDependency[Int, Int, Int] = _
+
+  private[this] val NUM_MAPS = 1
+  private[this] val NUM_PARTITITONS = 4
+  private[this] val REDUCE_ID = 1
+  private[this] val SHUFFLE_ID = 0
+  private[this] val conf = new SparkConf()
+  private[this] val memoryManager = new TestMemoryManager(conf)
+  private[this] val hashPartitioner = new HashPartitioner(NUM_PARTITITONS)
+  private[this] val serializer = new KryoSerializer(conf)
+  private[this] val spillFilesCreated = new JLinkedList[File]()
+  private[this] val taskMemoryManager = new TaskMemoryManager(memoryManager, 0)
+  private[this] val taskMetrics = new TaskMetrics()
+
+  private[this] var tempDir: File = _
+  private[this] var mergedOutputFile: File = _
+  private[this] var partitionSizesInMergedFile: Array[Long] = _
+  private[this] val ugi = UserGroupInformation.createUserForTesting("testuser", Array("testgroup"))
+
+  // Create a mocked shuffle handle to pass into HashShuffleReader.
+  private[this] val shuffleHandle = {
+    val dependency = mock(classOf[ShuffleDependency[Int, Int, Int]])
+    when(dependency.serializer).thenReturn(Some(serializer))
+    when(dependency.aggregator).thenReturn(None)
+    when(dependency.keyOrdering).thenReturn(None)
+    new BaseShuffleHandle(SHUFFLE_ID, NUM_MAPS, dependency)
+  }
+
+  // Make a mocked MapOutputTracker for the shuffle reader to use to determine what
+  // shuffle data to read.
+  private[this] val mapOutputTracker = mock(classOf[MapOutputTracker])
+  private[this] val sparkEnv = mock(classOf[SparkEnv])
+
+  override def beforeAll(): Unit = {
+    when(sparkEnv.conf).thenReturn(conf)
+    SparkEnv.set(sparkEnv)
+
+    System.setProperty("SPARK_YARN_MODE", "true")
+    ugi.doAs(new PrivilegedExceptionAction[Unit]() {
+      override def run(): Unit = {
+        conf.set(SPARK_SHUFFLE_ENCRYPTION_ENABLED, true.toString)
+        val creds = new Credentials()
+        CryptoConf.initSparkShuffleCredentials(conf, creds)
+        SparkHadoopUtil.get.addCurrentUserCredentials(creds)
+      }
+    })
+  }
+
+  override def afterAll(): Unit = {
+    SparkEnv.set(null)
+  }
+
+  override def beforeEach(): Unit = {
+    super.beforeEach()
+    MockitoAnnotations.initMocks(this)
+    tempDir = Utils.createTempDir()
+    mergedOutputFile = File.createTempFile("mergedoutput", "", tempDir)
+  }
+
+  override def afterEach(): Unit = {
+    super.afterEach()
+    conf.set("spark.shuffle.compress", false.toString)
+    Utils.deleteRecursively(tempDir)
+    val leakedMemory = taskMemoryManager.cleanUpAllAllocatedMemory()
+    if (leakedMemory != 0) {
+      fail("Test leaked " + leakedMemory + " bytes of managed memory")
+    }
+  }
+
+  test("yarn shuffle encryption read and write") {
+    ugi.doAs(new PrivilegedExceptionAction[Unit] {
+      override def run(): Unit = {
+        testYarnShuffleEncryptionWriteRead()
+      }
+    })
+  }
+
+  test("yarn shuffle encryption read and write with shuffle compression enabled") {
+    ugi.doAs(new PrivilegedExceptionAction[Unit] {
+      override def run(): Unit = {
+        conf.set("spark.shuffle.compress", true.toString)
+        testYarnShuffleEncryptionWriteRead()
+      }
+    })
+  }
+
+  private[this] def testYarnShuffleEncryptionWriteRead(): Unit = {
+    val dataToWrite = new JArrayList[Product2[Int, Int]]()
+    for (i <- 0 to NUM_PARTITITONS) {
+      dataToWrite.add((i, i))
+    }
+    val shuffleWriter = createWriter()
+    shuffleWriter.write(dataToWrite.iterator())
+    shuffleWriter.stop(true)
+
+    val shuffleReader = createReader()
+    val iter = shuffleReader.read()
+    val recordsList = new JArrayList[(Int, Int)]()
+    while (iter.hasNext) {
+      recordsList.add(iter.next().asInstanceOf[(Int, Int)])
+    }
+
+    assertEquals(HashMultiset.create(dataToWrite), HashMultiset.create(recordsList))
+  }
+
+  private[this] def createWriter(): UnsafeShuffleWriter[Int, Int] = {
+    initialMocksForWriter()
+    new UnsafeShuffleWriter[Int, Int](
+      blockManager,
+      blockResolver,
+      taskMemoryManager,
+      new SerializedShuffleHandle[Int, Int](SHUFFLE_ID, NUM_MAPS, shuffleDep),
+      0, // map id
+      taskContext,
+      conf
+    )
+  }
+
+  private[this] def createReader(): BlockStoreShuffleReader[Int, Int] = {
+    initialMocksForReader()
+    new BlockStoreShuffleReader(
+      shuffleHandle,
+      REDUCE_ID,
+      REDUCE_ID + 1,
+      TaskContext.empty(),
+      blockManager,
+      mapOutputTracker)
+  }
+
+  private[this] def initialMocksForWriter(): Unit = {
+    when(blockManager.diskBlockManager).thenReturn(diskBlockManager)
+    when(blockManager.conf).thenReturn(conf)
+    when(blockManager.getDiskWriter(any(classOf[BlockId]), any(classOf[File]),
+      any(classOf[SerializerInstance]), anyInt, any(classOf[ShuffleWriteMetrics]))).thenAnswer(
+          new Answer[DiskBlockObjectWriter]() {
+            override def answer(invocationOnMock: InvocationOnMock): DiskBlockObjectWriter = {
+              val args = invocationOnMock.getArguments
+              new DiskBlockObjectWriter(args(1).asInstanceOf[File],
+                args(2).asInstanceOf[SerializerInstance],
+                args(3).asInstanceOf[Integer], new CompressStream(), false,
+                args(4).asInstanceOf[ShuffleWriteMetrics], args(0).asInstanceOf[BlockId],
+                conf)
+            }
+          })
+
+    when(blockResolver.getDataFile(anyInt(), anyInt())).thenReturn(mergedOutputFile)
+    doAnswer(new Answer[Unit]() {
+      override def answer(invocationOnMock: InvocationOnMock): Unit = {
+        partitionSizesInMergedFile = invocationOnMock.getArguments()(2).asInstanceOf[Array[Long]]
+        val tmp = invocationOnMock.getArguments()(3)
+        mergedOutputFile.delete()
+        tmp.asInstanceOf[File].renameTo(mergedOutputFile)
+      }
+    }).when(blockResolver).writeIndexFileAndCommit(anyInt(), anyInt(), any(classOf[Array[Long]]),
+         any(classOf[File]))
+
+    when(diskBlockManager.createTempShuffleBlock()).thenAnswer(
+      new Answer[(TempShuffleBlockId, File)]() {
+        override def answer(invocationOnMock: InvocationOnMock): (TempShuffleBlockId, File) = {
+          val blockId = new TempShuffleBlockId(UUID.randomUUID())
+          val file = File.createTempFile("spillFile", ".spill", tempDir)
+          spillFilesCreated.add(file)
+          (blockId, file)
+        }
+      })
+
+    when(taskContext.taskMetrics()).thenReturn(taskMetrics)
+    when(shuffleDep.serializer).thenReturn(Option.apply(serializer))
+    when(shuffleDep.partitioner).thenReturn(hashPartitioner)
+    when(taskContext.taskMetrics()).thenReturn(taskMetrics)
+    when(taskContext.internalMetricsToAccumulators).thenReturn(null)
+  }
+
+  private[this] def initialMocksForReader(): Unit = {
+    // Create a return function to use for the mocked wrapForCompression method to initial a
+    // compressed input stream if spark.shuffle.compress is enabled
+    val compressionFunction = new Answer[InputStream] {
+      override def answer(invocation: InvocationOnMock): InputStream = {
+        if (conf.getBoolean("spark.shuffle.compress", false)) {
+          CompressionCodec.createCodec(conf).compressedInputStream(
+            invocation.getArguments()(1).asInstanceOf[InputStream])
+        } else {
+          invocation.getArguments()(1).asInstanceOf[InputStream]
+        }
+      }
+    }
+
+    // Setup the mocked BlockManager to return RecordingManagedBuffers.
+    val localBlockManagerId = BlockManagerId("test-client", "test-client", 1)
+    when(blockManager.blockManagerId).thenReturn(localBlockManagerId)
+
+    var startOffset = 0L
+    for (mapId <- 0 until NUM_PARTITITONS) {
+      val bytes = new Array[Byte](partitionSizesInMergedFile(mapId).toInt)
+      val in = new FileInputStream(mergedOutputFile)
+      try {
+        ByteStreams.skipFully(in, startOffset)
+        in.read(bytes)
+      } finally {
+        in.close()
+      }
+      // Create a ManagedBuffer with the shuffle data.
+      val nioBuffer = new NioManagedBuffer(ByteBuffer.wrap(bytes))
+      val managedBuffer = new RecordingManagedBuffer(nioBuffer)
+      startOffset += partitionSizesInMergedFile(mapId)
+      // Setup the blockManager mock so the buffer gets returned when the shuffle code tries to
+      // fetch shuffle data.
+      val shuffleBlockId = ShuffleBlockId(SHUFFLE_ID, mapId, REDUCE_ID)
+      when(blockManager.getBlockData(shuffleBlockId)).thenReturn(managedBuffer)
+      when(blockManager.wrapForCompression(meq(shuffleBlockId), isA(classOf[InputStream])))
+        .thenAnswer(compressionFunction)
+    }
+
+    // Test a scenario where all data is local, to avoid creating a bunch of additional mocks
+    // for the code to read data over the network.
+    val shuffleBlockIdsAndSizes = (0 until NUM_PARTITITONS).map { mapId =>
+      val shuffleBlockId = ShuffleBlockId(SHUFFLE_ID, mapId, REDUCE_ID)
+      (shuffleBlockId, partitionSizesInMergedFile(mapId))
+    }
+    val mapSizesByExecutorId = Seq((localBlockManagerId, shuffleBlockIdsAndSizes))
+    when(mapOutputTracker.getMapSizesByExecutorId(SHUFFLE_ID, REDUCE_ID, REDUCE_ID + 1)).thenReturn
+    {
+      mapSizesByExecutorId
+    }
+  }
+
+  private[this] final class CompressStream extends AbstractFunction1[OutputStream, OutputStream] {
+    override def apply(stream: OutputStream): OutputStream = {
+      if (conf.getBoolean("spark.shuffle.compress", false)) {
+        CompressionCodec.createCodec(conf).compressedOutputStream(stream)
+      } else {
+        stream
+      }
+    }
+  }
+}
-- 
1.7.9.5

